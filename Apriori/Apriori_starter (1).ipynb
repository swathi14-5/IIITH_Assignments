{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Apriori_starter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPMjnwE_WN2i",
        "outputId": "a876f3a9-59cc-4044-deb2-253aaee20a1f"
      },
      "source": [
        "from collections import defaultdict\n",
        "import itertools\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import combinations as subset\n",
        "import time\n",
        "\n",
        "# Kindly add datasets to this list if you wish to run the program on more dataset\n",
        "datasets = ['BMS1_spmf.txt']\n",
        "min_support = 0.01\n",
        "#min_support = 0.08\n",
        "#min_support = 0.04\n",
        "#min_support = 0.05\n",
        "level = 1\n",
        "#To obtain the transactions for the level1\n",
        "def get_level1_items(transactions):\n",
        "    search_items = set()\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            search_items.add(item)\n",
        "    items =  [[item] for item in search_items]\n",
        "    return items\n",
        "\n",
        "def all_except_last(item_1, item_2):\n",
        "    if item_1[:-1] == item_2[:-1] and item_1[-1] < item_2[-1]: return True\n",
        "    return False\n",
        "\n",
        "def has_infrequent_subset(item, L_prev):\n",
        "    subsets = [sorted(list(s)) for s in (subset(item, len(item) - 1))]\n",
        "    for s in subsets: \n",
        "        if s not in L_prev: return True\n",
        "    return False\n",
        "\n",
        "def apriori_gen(L_prev):\n",
        "    C_ret = []\n",
        "    for item_1 in L_prev:\n",
        "        for item_2 in L_prev:\n",
        "            if all_except_last(item_1,item_2):\n",
        "                item = item_1 + [item_2[-1]]\n",
        "                if not has_infrequent_subset(item, L_prev):\n",
        "                    C_ret.append(item)\n",
        "    return C_ret\n",
        "\n",
        "## This method not only identifies the transactions which  \n",
        "## have mininum support and also keeps only throse transactions \n",
        "## so that further processing will be easy\n",
        "def _find_frequent_1_itemsets(transactions, min_sup):\n",
        "    trimmed_trans = []\n",
        "    items_set = set()\n",
        "    result_items_map =defaultdict(list)\n",
        "    \n",
        "    ## First all items into set.\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            items_set.add(item)\n",
        "    \n",
        "    for item in items_set:\n",
        "        tran_array_indexs = []\n",
        "        tran_counter = -1\n",
        "        for transaction in transactions:\n",
        "            tran_counter = tran_counter + 1\n",
        "            if item in transaction:\n",
        "                tran_array_indexs.append(tran_counter)\n",
        "\n",
        "            \n",
        "        if(len(tran_array_indexs)>=min_sup):\n",
        "            result_items_map[item] = tran_array_indexs\n",
        "    \n",
        "    \n",
        "    final_items = [[item] for item in result_items_map.keys()]\n",
        "    \n",
        "    final_tran_array_indexs = set(itertools.chain(*result_items_map.values()))\n",
        "    \n",
        "    \n",
        "    for i in final_tran_array_indexs:\n",
        "        trimmed_trans.append(transactions[i])\n",
        "    \n",
        "    return (final_items , trimmed_trans)\n",
        "\n",
        "\n",
        "def _find_frequentitemsets2(transactions, items_to_search, min_sup,curr_level):\n",
        "    trimmed_trans = []\n",
        "    \n",
        "    result_items_map =defaultdict(list)\n",
        "    \n",
        "    for item in items_to_search:\n",
        "        tran_array_indexs = []\n",
        "        tran_counter = -1\n",
        "        for transaction in transactions:\n",
        "            tran_counter = tran_counter + 1\n",
        "            isFound = set(item).issubset(set(transaction))\n",
        "            if isFound:\n",
        "                tran_array_indexs.append(tran_counter)\n",
        "        cnt_transacitons = len(tran_array_indexs)\n",
        "        #print(f\" searching for {item} in all trans and found in  transactions {cnt_transacitons}\") \n",
        "            \n",
        "        if(cnt_transacitons>=min_sup):\n",
        "            result_items_map[tuple(item)] = tran_array_indexs\n",
        "    \n",
        "    \n",
        "            \n",
        "    if(curr_level>1):\n",
        "        final_items = [list(item) for item in result_items_map.keys()]\n",
        "    else:\n",
        "        final_items = [[item[0]] for item in result_items_map.keys()]\n",
        "    \n",
        "    final_tran_array_indexs = set(itertools.chain(*result_items_map.values()))\n",
        "    \n",
        "    \n",
        "    for i in final_tran_array_indexs:\n",
        "        trimmed_trans.append(transactions[i])\n",
        "    \n",
        "    return (final_items , trimmed_trans)\n",
        "\n",
        "def generate_frequent_itemsets_2(transactions, freq_items, min_sup):\n",
        "\t## level maintains the level\n",
        "    global level\n",
        "    min_sup_count\t=\tmin_sup * len(transactions)\n",
        "    print(f\" For Level = {level} total transacitons = {len(transactions)}, input min_sup={min_sup}, min_sup% = {min_sup*100} and min_sup_count = {min_sup_count}\")\n",
        "\t\n",
        "    fi_trans    \t\t= _find_frequentitemsets2(transactions,freq_items, min_sup_count,level)\n",
        "    freq_items    \t    = fi_trans[0]\n",
        "    transactions \t\t= fi_trans[1]\n",
        "    transactions \t\t= list(filter(lambda x: len(x)>=level, transactions))\n",
        "    prev_freq_items \t= freq_items\n",
        "    while len(freq_items) > 0:\n",
        "        prev_freq_items =  freq_items\n",
        "        freq_items \t    =  apriori_gen(freq_items)\n",
        "        ##print(f\"freq_items={freq_items[0]}\")\n",
        "        min_sup_count = len(transactions) * min_sup\n",
        "        level = level + 1\n",
        "        fi_trans = _find_frequentitemsets2(transactions,freq_items, min_sup_count,level)\n",
        "        freq_items    \t= fi_trans[0]\n",
        "        transactions    = fi_trans[1]\n",
        "        transactions    = list(filter(lambda x: len(x)>=level, transactions))\n",
        "        \n",
        "        print(f\"level={level-1} and FI length={len(freq_items)} and freq_items = {prev_freq_items}\")\n",
        "        print(\"-----------------------------------------------------------------------------------\")\n",
        "    return prev_freq_items\n",
        "\n",
        "\n",
        "def process(data):\n",
        "    data = data.split(\"\\n\")\n",
        "    if not data[-1]: data = data[:-1]\n",
        "    data= [set([int(item.strip()) for item in line.split(\"-2\")[0].strip().split(\"-1\")[:-1]]) for line in data]\n",
        "    \n",
        "    return data\n",
        "\n",
        "for dataset in datasets:\n",
        "    with open(dataset) as f:\n",
        "        data = f.read()\n",
        "    transactions = process(data)\n",
        "    items = get_level1_items(transactions)\n",
        "    print(\"Dataset:\", datasets)\n",
        "    print(\"Runing Apriori Algorithm\")\n",
        "    start_time = time.time()\n",
        "    apriori_result = generate_frequent_itemsets_2(transactions,items,min_support)\n",
        "    print(\"Time taken:\", time.time() - start_time)\n",
        "\n",
        "    print(\"At level \" , level-1, \" Frequent itemsets formed for\", dataset, \"at min_support\", min_support*100, \"%:\")\n",
        "    print(\"No Items are found for the next level\")\n",
        "    print(\"*************************************************************************************\")\n",
        "    print(\"This is the final output set for the transaction:\",apriori_result)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: ['BMS1_spmf.txt']\n",
            "Runing Apriori Algorithm\n",
            " For Level = 1 total transacitons = 59601, input min_sup=0.08, min_sup% = 8.0 and min_sup_count = 4768.08\n",
            "Time taken: 13.745901346206665\n",
            "At level  0  Frequent itemsets formed for BMS1_spmf.txt at min_support 8.0 %:\n",
            "No Items are found for the next level\n",
            "*************************************************************************************\n",
            "This is the final output set for the transaction: []\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}